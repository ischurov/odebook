\chapter Многомерные линейные уравнения с постоянными коэффициентами
    \label chap:11:multidim-linear

\section Оператор монодромии 

Рассмотрим линейное уравнение
\equation \label eq:11:main
    \dot{\mb z}=A\mb z,\quad \mb z(t)\in\mathbb R^n

Ранее мы разобрали случаи уравнений на плоскости (когда $n=2$). Сегодня мы рассмотрим общий случай.

Когда мы рассматривали линейные системы на плоскости, у нас получалось, что решение задается в виде

\eq
    \mb z(t)=M(t) \mb z^0,
где $M(t)$ — некоторая матрица (зависящая от $t$), причём $M(0)=E$: см. уравнения \ref{eq:10:z-of-t} и \ref{eq:10:a-o-sol} \ref[предыдущей главы\nonumber][chap:10:linear-systems]. Это неспроста.

Пусть $\varphi(t;\mb z^0)$ — решение уравнения \ref{eq:11:main} с начальным условием $\mb z(0)=\mb z^0$. Зафиксируем некоторое $t$ и рассмотрим \em{отображение последования} (оно также называется \em{преобразованием фазового потока}) за время $t$:

\eq
    g^t(\mb z^0)=\varphi(t;\mb z^0)

То есть для каждой точки фазового пространства $z^0$ отобразим её туда, где она окажется через время $t$. 

\example
    Рассмотрим систему 
    \eq
        \dot x = 2x, \quad \dot y = 3y
    Для неё 
    \eq
        g^1
        \begin{pmatrix} 
            x_0 \\\\ 
            y_0 
        \end{pmatrix} 
        = 
        \begin{pmatrix} 
            e^2 & 0 \\\\ 
            0 & e^3  
        \end{pmatrix}\cdot
        \begin{pmatrix} 
            x_0 \\\\ 
            y_0 
        \end{pmatrix},
    а в общем виде 
    \eq
        g^t
        \begin{pmatrix} 
            x_0 \\\\ 
            y_0 
        \end{pmatrix} 
        = 
        \begin{pmatrix} 
            e^{2t} & 0 \\\\
            0 & e^{3t} 
        \end{pmatrix} 
        \begin{pmatrix}
            x_0\\\\
            y_0
        \end{pmatrix}
    или просто 
    \eq
        g^t=
        \begin{pmatrix} 
            e^{2t} & 0 \\\\ 
            0 & e^{3t} 
        \end{pmatrix} 

\theorem 
    Для системы \ref{eq:11:main} и любого $t$ отображение $g^t$ является линейным.

\proof
    Пусть $\mb u$ и $\mb v$ — некоторые векторы из фазового пространства
    $\mathbb R^n$. Тогда

    \eq
        g^t(\mb u+\mb v)=\varphi(t;\mb u+ \mb v)

    Пусть $\psi(t)=\varphi(t;\mb u)+\varphi(t;\mb v)$. Функция $\psi$ является
    решением уравнения \ref{eq:11:main} (в силу линейности). При этом
    $\psi(0)=\varphi(0;\mb u)+\varphi(0;\mb v)=\mb u+\mb v$. Значит, это решение
    с начальным условием $\mb u+\mb v$. Но решение с начальным условием $\mb
    u+\mb v$ задаётся функцией $\varphi(t;\mb u+\mb v)$. Значит (по теореме о
    существовании и единственности), $\psi(t)=\varphi(t;\mb u+\mb v)$. Таким
    образом,

    \eq
        g^t(\mb u+\mb v)=\varphi(t;\mb u)+\varphi(t; \mb v)=g^t (\mb u)+g^t (\mb v)

    Аналогично проверяется и вторая аксиома линейности для $g^t$. (Упражнение:
    завершить доказательство.)


\corollary 
    Пространство решений линейного дифференциального уравнения имеет такую же
    размерность, как фазовое пространство.

\section Матричная экспонента

Таким образом, для решения уравнения \ref{eq:11:main} нам нужно будет найти
матрицу $M(t)$ (она иногда называется «матрицей монодромии»), которая задаёт
решение. Как её найти? Если бы $A$ была не матрицей, а числом (вещественным и
комплексным), мы бы мгновенно записали решение в виде экспоненты. Нельзя ли с
матрицей сделать то же самое, то есть записать решение уравнения
\ref{eq:11:main} в виде \em{экспоненты от матрицы}?

\equation \label eq:11:eAt
    \mb z(t)=e^{At} \mb z^0

На первый взгляд, это кажется безумием. (Хотя возможно вас с ним уже познакомили на курсе линейной алгебры.) Что значит «возвести число $e$ в степень матрицы»? Ерунда какая-то. Впрочем, не большая ерунда, чем возведение числа $e$ в степень $\pi$ (вы ведь помните, что изначально возвести число в степень — это умножить его на себя сколько-то раз — как это можно сделать иррациональное число раз?). Так что может быть и с матрицей получится?

Напомним, что экспоненту от числа $x$ можно определить как сумму ряда

\eq
    e^x=\sum_{n=0}^{\infty} \frac{x^n}{n!}

Оказывается, нет ничего невозможного в том, чтобы подставить в этот ряд матрицу $A$. Действительно, чтобы посчитать значение этого ряда, нам нужно только уметь складывать матрицы и возводить их в натуральные степени — а это мы делать умеем. Итак, по определению,

\equation \label eq:11:eA
    e^A=\sum_{n=0}^{\infty} \frac{A^n}{n!}, \quad A^0=E

Вы, безусловно, изучали анализ, и при взгляде на такую запись у вас неминуемо должен возникнуть вопрос: а почему этот ряд сходится? И всегда ли он сходится? Оказывается, да, сходится, причём всегда и, более того, абсолютно сходится. Мы не будем давать аккуратного доказательства, но наметим его основные шаги:

\enumerate
    \item Ввести норму на пространстве линейных операторов. Делается это так:
        пусть $\\|\mb v\\|$ — норма вектора $\mb v$ (то есть на фазовом
        пространстве какая-то норма введена). В этом случае определим норму
        оператора $A$ следующим образом:
        \eq
            \\|A\\| = \sup_{\mb v\colon \\|\mb v\\| = 1} \\|A \mb v\\|
    \item 
        Доказать, что эта норма обладает следующим свойством:
        \eq
            \\|A \cdot B \\| \le \\|A \\| \cdot \\| B \\|.
    \item 
        Доказать, что каждое слагаемое ряда \ref{eq:11:eA} ограничено сверху
        по норме соответствующим слагаемым для ряда $e^{\\|A\\|}$.
    \item 
        Ряд для $e^{\\|A\\|}$ сходится абсолютно, а значит и ряд
        \ref{eq:11:eA}  тоже сходится абсолютно.


\subsection Матричная экспонента даёт решение линейного уравнения

Покажем, функция, заданная формулой \ref{eq:11:eAt}, является решением уравнения
\ref{eq:11:main}. Действительно,

\eq
    \frac{d}{dt}e^{At}\mb z^0=\frac{d}{dt}\sum_{n=0}^{\infty} \frac{t^n A^n \mb z^0}{n!}=\sum_{n=1}^\infty \frac{nt^{n-1} A^n \mb z^0}{n!}=A\sum_{n=0}^\infty \frac{t^n A^n}{n!}=Ae^{At}\mb z^0.
Дифференцирование ряда допустимо, поскольку он сходится абсолютно (чего мы правда не доказали).

\question
    Правда ли, что $e^{A+B}=e^A e^B$?
    \quiz
        \choice Правда, потому что это верно для обычной экспоненты
            \comment Ну тогда попробуйте это доказать.
        \choice Неправда. \correct
            \comment Действительно, в отличие от чисел, матрицы не обязаны
                коммутировать по умножению. Поэтому ожидать выполнения этого
                равенства кажется странно: его левая часть не меняется от того, что
                мы поменяем $A$ и $B$ местами, а правая часть может и измениться.
                Подобрать теперь конкретный контрпример не очень сложно: сделайте
                это самостоятельно. На самом деле, именно отсутствие коммутирования
                для матриц не даёт нам доказать это утверждение.

\subsection Нахождение матричной экспоненты

\proposition
    Справедливо следующее соотношение
    \eq
        e^{CAC^{-1}}=Ce^{A}C^{-1}
    Оно означает, что вычисление экспоненты корректно определено на пространстве
    операторов: оно «дружит» с заменой базиса.

\proof
    Имеем:
    \eq
        Ce^AC^{-1} = C\left( \sum_{n=0}^{\infty} \frac{ A^n}{n!}\right) C^{-1}
        = \sum_{n=0}^{\infty} \frac{ C A^n C^{-1}}{n!}=e^{CAC^{-1}} 

Это означает, что для нахождения экспоненты можно перейти в «хороший» базис
(например, собственный или на худой конец жорданов), найти экспоненту там, а
затем перейти в исходный базис. Рассмотрим теперь два случая.

\subsection Экспонента диагонализируемой матрицы

Пусть $A$ диагонализируема, то есть существует такая матрица $C$, что  $CA C^{-1}$ диагональная. Обозначим её за $D$. Тогда $A=C^{-1}DC$. Заметим, что
\eq
    \exp \begin{pmatrix} 
        \lambda_1 &\dots& 0 \\\\ 
        \vdots& \ddots & \vdots \\\\ 
        0& \dots & \lambda_n 
    \end{pmatrix} 
    =
    \begin{pmatrix} 
        \sum_{k=0}^\infty \frac{\lambda_1^k}{k!} &\dots& 0 \\\\
        \vdots& \ddots & \vdots \\\\
        0& \dots & \sum_{k=0}^\infty \frac{\lambda_n^k}{k!} 
    \end{pmatrix}  
    =
    \begin{pmatrix} 
        e^{\lambda_1} &\dots& 0 \\\\
        \vdots& \ddots & \vdots \\\\ 
        0& \dots & e^{\lambda_n }
    \end{pmatrix} 
Решение уравнения \ref{eq:11:main} для диагонализируемой матрицы
$A$ теперь представляется в виде: 
\eq
    \mb z(t) = \exp(At) \mb z^0 = C^{-1} \cdot \exp(Dt) \cdot C \cdot \mb z^0,
где $\mb z^0$ — начальное условие.

\subsection Экспонента жордановой клетки

Если матрица не диагонализируема, то она по крайней мере приводится к жордановой
нормальной форме. Рассмотрим случай жордановой клетки.

Пусть
\eq
    J_\lambda = 
    \begin{pmatrix} 
        \lambda & 1 &\dots& 0 \\\\ 
        \vdots& \ddots & \ddots & \vdots \\\\ 
        \vdots& \dots & \lambda &1 \\\\ 
        0& \dots & 0 & \lambda 
    \end{pmatrix}
Чтобы найти $e^{J_\lambda}$, заметим для начала, что
\eq
    J_\lambda = \lambda E + N,
где $N$ — это нильпотентный оператор, матрица которого состоит из единичек на
диагонали, сдвинутой на 1 вверх. 

Заметим, что $\lambda E$ коммутирует с любой другой матрицей, в том числе и с
матрицей $N$. Можно показать (мы этого не делали), что благодаря коммутированию
\eq
    e^{\lambda E + N} = e^{\lambda E} e^N = e^\lambda e^N.

Однако матрица $N$ нильпотентная, то есть существует такое $k$, что
$N^k=0$. В этом случае ряд для экспоненты имеет лишь конечное число ненулевых
слагаемых и становится полиномом: 
\eq
    \sum_{i=0}^\infty \frac{N^i}{i!} = \sum_{i=0}^k \frac{N^i}{i!}
Его можно посчитать явно, а значит можно найти экспоненту от жордановой клетки.

\remark 
    Для нашего рассуждения требовалась только нильпотентность матрицы $N$: тот
    факт, что она состоит именно из единиц, не является принципиальным. В
    частности, абсолютно аналогично можно вычислять экспоненту от матрицы,
    пропорциональной жордановой клетке.

\example 
    Рассмотрим линейное уравнение на плоскости с оператором $A$, имеющим
    совпадающие собственные значения (равные $\lambda$), но не являющимся
    диагональным. Для нахождения решения по формуле \ref{eq:11:eAt} нам
    потребуется найти экспоненту от матрицы, пропорциональной жордановой клетке:

    \eq
        \exp 
        \left( 
            \begin{pmatrix} 
                \lambda & 1 \\\\ 
                0 & \lambda \end{pmatrix} 
            t
        \right) 
        = 
        \exp 
        \left( 
            \begin{pmatrix} 
                \lambda t & 0 \\\\ 
                0 & \lambda t
            \end{pmatrix} \right) 
        \cdot 
        \left[
            \begin{pmatrix}
                1 & 0 \\\\
                0 & 1
            \end{pmatrix}
            +
            \frac{1}{1!}
            \begin{pmatrix}
                0 & t \\\\
                0 & 0
            \end{pmatrix}
        \right]
        =
        e^{\lambda t}
        \begin{pmatrix} 
            1 & t \\\\
            0 & 1
        \end{pmatrix} 
        = 
        \begin{pmatrix} 
            e^{\lambda t} & t e^{\lambda t} \\\\ 
            0 & e^{\lambda t} 
        \end{pmatrix} 
    Фазовый портрет этой системы см. на \ref[рис.][fig:10:node-degenerate].

\subsection Общий случай

Пусть ЖНФ матрицы имеет несколько клеток. Каждая из них соответствует некоторому
инвариантному подпространству линейного оператора: оператор действует на
различных инвариантных подпространствах независимо. Это означает, что мы можем
вычислить экспоненту от каждой жордановой клетки и затем составить большую
блочно-диагональную матрицу, состоящую из таких же блоков, как исходная (в ЖНФ).

Таким образом, мы можем вычислить экспоненту от любой матрицы — самым сложным
этапом при этом является её приведение к ЖНФ и отыскание жорданова базиса.

Значит, мы умеем решать любые системы линейных уравнений!
